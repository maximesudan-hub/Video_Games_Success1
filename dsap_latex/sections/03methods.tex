% ============================
\section{Methodology}
% ============================

% ==============================
\subsection{Data Description}
% ==============================

The dataset contains 16,000+ video games released between 1980 and 2016 and is sourced from Kaggle's \emph{Video Game Sales with Ratings} \cite{KaggleVGSales}. We focus on modern games (2005+) and remove entries with missing essential metadata. Key variables include global sales (millions of units), critic and user scores/counts (Metacritic), platform, genre, publisher, developer, rating, and release year. The target for regression is $\log(1+\text{Global\_Sales})$ to reduce skewness.

% ==============================
\subsection{Approach}
% ==============================

\paragraph{Algorithms}
Core models are supervised regressions and classifiers: OLS and Ridge as linear baselines, Random Forest and HistGradientBoosting for nonlinear regression, and Logistic Regression plus Random Forest Classifier for success prediction. A complementary K-means clustering on numeric features (scores, counts, release year) groups games into interpretable segments.

\paragraph{Preprocessing}
Numeric features are imputed (median) and scaled when required. Categorical features are one-hot encoded for linear models and ordinal-encoded for tree models. We use an 80/20 train/test split with fixed random seed (\texttt{random\_state} = 42), and define success as Global\_Sales $> 1$ million units.

This success threshold follows common industry discussions of “hit” games, but it creates class imbalance because only a minority of titles exceed 1M units. We therefore report ROC-AUC alongside precision/recall to reflect performance under imbalance and to avoid over-reliance on a single decision threshold.

\paragraph{Feature selection and leakage control}
Sales variables (Global\_Sales and regional sales) are excluded from the feature set to avoid leakage. The model uses descriptive metadata (platform, genre, publisher, developer, rating, release year) and reception indicators (critic and user scores/counts). Review counts and scores are post-release signals, so they are treated as predictive indicators rather than causal drivers.

\paragraph{Target transformation}
Global sales are highly skewed, so regression targets use $\log(1+\text{Global\_Sales})$ to stabilize variance and improve model fit. This transformation reduces the influence of extreme blockbuster titles.
\paragraph{Why log1p(Global\_Sales)}
Sales distributions are heavy-tailed: a small number of blockbuster titles dominate the upper tail. Using $\log(1+\text{Global\_Sales})$ compresses extreme values and makes the target closer to a normal distribution, which benefits both linear models and tree-based regressors. The transformation also yields errors that are closer to relative (percentage) deviations rather than absolute units, which is more meaningful when comparing low- and high-selling games.

\paragraph{Model Architecture}
\begin{itemize}
\item Regression: fit pipelines, predict Log\_Sales, evaluate with RMSE and $R^2$.
\item Classification: fit pipelines, predict success, evaluate with Accuracy/Precision/Recall/F1 and ROC-AUC.
\item Clustering: numeric-only K-means after imputation and standardization; PCA for 2D visualization.
\end{itemize}

\paragraph{Evaluation protocol}
All metrics are computed on the held-out test set. For classification, ROC-AUC is reported to avoid reliance on a single threshold. Figures include a prediction-vs-true scatter (regression), a ROC curve (classification), and PCA visualizations for clustering.

\paragraph{Evaluation Metrics}
\begin{itemize}
\item Regression: RMSE and $R^2$ on the test set.
\item Classification: Accuracy, Precision, Recall, F1, and ROC-AUC.
\item Interpretation: Random Forest feature importance (post-encoding).
\end{itemize}

\paragraph{Why this pipeline answers the research question}
The research question is explanatory, but the evidence is empirical and predictive. The pipeline first estimates sales outcomes, then identifies which variables drive predictive performance (feature importance), and finally checks whether distinct game segments emerge through clustering. This combination provides both quantitative performance and an interpretable ranking of factors, which directly addresses the question of what explains commercial success.

\paragraph{Model comparison rationale}
We report RMSE/$R^2$ for regression and Accuracy/Precision/Recall/F1 plus ROC-AUC for classification to balance error size and ranking ability under class imbalance. Linear models provide a transparent baseline, while tree-based models capture nonlinear interactions (platform $\times$ engagement $\times$ timing). Regression captures continuous sales; classification targets the practical threshold of 1M units.
% ==============================
\paragraph{Exploratory clustering}
We use an unsupervised approach to summarize heterogeneity across games. \textbf{Clustering is purely exploratory and does not feed into predictive models; it is included only to provide descriptive intuition.} Clustering uses numeric indicators (release year, critic score, critic count, user score, user count) with median imputation and standardization, and we select $K=3$ to balance interpretability and separation.

% ==============================
\subsection{Implementation}
% ==============================

The project is implemented in Python using \texttt{scikit-learn}, \texttt{pandas}, and \texttt{numpy}. The codebase is modular, with separate modules for data preprocessing, model training, and evaluation.

\paragraph{Pipeline structure}
Each model is wrapped in a \texttt{Pipeline} with a \texttt{ColumnTransformer} that handles numeric and categorical features. This ensures consistent preprocessing, prevents leakage, and allows direct comparison across algorithms.

\paragraph{Outputs and reproducibility}
All metrics are saved to \texttt{results/metrics/} and figures to \texttt{results/figures/}. The project is runnable via \texttt{python main.py}; preprocessing can be reproduced with \texttt{python -m src.data\_loader}. Dependencies are listed in \texttt{requirements.txt} and all stochastic models use fixed seeds (e.g., \texttt{random\_state=42}).

\paragraph{Implementation challenges}
The dataset mixes numeric signals and categorical fields, so separate preprocessing pipelines are used to impute, scale, and encode variables without leakage. Different algorithms require different encodings (one-hot for linear models vs.\ ordinal for tree models), and interpretability is addressed by reporting Random Forest feature importance alongside the best predictive model. For traceability, the pipeline logs key dataset statistics and stores all metrics/figures deterministically, which helps align the report tables with the generated outputs.
